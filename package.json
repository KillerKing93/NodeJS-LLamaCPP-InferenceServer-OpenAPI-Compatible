{
  "name": "nodejs-llamacpp-inferenceserver-openapi-compatible",
  "version": "1.0.0",
  "description": "OpenAI-compatible inference server using node-llama-cpp and automated model lifecycle (download -> convert GGUF -> serve).",
  "main": "index.js",
  "type": "commonjs",
  "scripts": {
    "start": "node index.js",
    "dev": "node --watch index.js",
    "download-model": "node scripts/download-model.js",
    "convert-model": "node scripts/convert-model.js",
    "setup-model": "node scripts/setup-model.js",
    "postinstall": "node scripts/postinstall-checks.js",
    "test": "echo \"No tests yet\" && exit 0"
  },
  "keywords": [],
  "author": "",
  "license": "SEE LICENSE IN LICENSE",
  "engines": {
    "node": ">=20"
  },
  "dependencies": {
    "dotenv": "^16.4.5",
    "express": "^5.1.0",
    "node-llama-cpp": "^3.14.0"
  }
}
