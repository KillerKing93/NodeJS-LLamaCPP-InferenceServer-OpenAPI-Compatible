# Server
PORT=3000

# Use prebuilt GGUF model (text-only)
MODEL_REPO_ID=TheBloke/Qwen2.5-1.5B-Instruct-GGUF
MODELS_DIR=./models
# Choose a widely available quant for low RAM usage
OUT_GGUF_NAME=Qwen2.5-1.5B-Instruct.Q4_K_M.gguf
# Point directly to the expected local path so server can load immediately after download
MODEL_GGUF_PATH=./models/TheBloke/Qwen2.5-1.5B-Instruct-GGUF/Qwen2.5-1.5B-Instruct.Q4_K_M.gguf

# Inference parameters
CTX_SIZE=4096
MAX_TOKENS=256
TEMPERATURE=0.7

# Download behavior for a GGUF repo
HF_FILE_PATTERNS=Qwen2.5-1.5B-Instruct.Q4_K_M.gguf
DOWNLOAD_CONCURRENCY=3

# Conversion tooling (not used for prebuilt GGUF, but kept for compatibility)
PYTHON_EXE=python
LLAMACPP_DIR=./tools/llama.cpp
CONVERT_ARGS=--outtype q8_0