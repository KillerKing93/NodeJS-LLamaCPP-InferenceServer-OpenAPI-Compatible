# Server
PORT=3000

# Model selection (easily switch models here)
# Default requested model (TensorFlow FP8 VLM; conversion may be unsupported for GGUF)
MODEL_REPO_ID=Qwen/Qwen3-VL-2B-Thinking-FP8
MODELS_DIR=./models
OUT_GGUF_NAME=model.gguf

# Optional: point directly to a GGUF file (overrides MODEL_REPO_ID/MODELS_DIR/OUT_GGUF_NAME)
# MODEL_GGUF_PATH=./models/Qwen/Qwen3-VL-2B-Thinking-FP8/model.gguf

# Inference parameters
CTX_SIZE=4096
MAX_TOKENS=256
TEMPERATURE=0.7

# Hugging Face access (optional)
# HF_TOKEN=               # set if the repo is gated/private

# Download behavior (optional)
DOWNLOAD_CONCURRENCY=3
# Limit downloads to certain patterns (comma-separated, simple * wildcard)
# For TF SavedModel-based repos:
HF_FILE_PATTERNS=saved_model.pb,variables/*,*.json,*.md,*.txt,*.model,*.bin,*.safetensors
# For Transformers (PyTorch) repos you might use:
# HF_FILE_PATTERNS=*.json,*.md,*.safetensors,*.bin

# Conversion tooling (optional)
PYTHON_EXE=python
LLAMACPP_DIR=./tools/llama.cpp
# If you already have convert-hf-to-gguf.py, set its full path:
# LLAMACPP_CONVERTER=
# Extra args to converter (example quantization type)
CONVERT_ARGS=--outtype q8_0

# Future option: auto-run setup on server start if model missing
# AUTO_SETUP=1